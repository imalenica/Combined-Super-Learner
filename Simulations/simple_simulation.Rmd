---
title: "Time Series Simulation"
author: "Rachael Phillips"
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this project, we will be in a setting of multi-person multi-dimensional time
series. We will use an online SL and can incorporate learners which pool
across subjects and individualize subjects (via more aggressive modeling of the
effect of the baseline covariates). The online cross-validated risk is
individualized, but we will average across subjects because it’s a more
interpretable measure.

# Simulation Overview

Simplistic settings which can capture the following:

* A situation where subjects are iid (baseline cov variation is null)
* Time series variation is very much a function of baseline covariates
* Time series variation is very much a function of baseline covariates that
you don’t measure (unexplained heterogeneity).
* Across time the process is stationary
* Time is not stationary (sudden jumps) so you cannot learn from the past how
much it will change in the future.

**Today I simulate from relatively simplistic ARIMA models.**

**Later I will model the binary or continuous MIMIC outcome data with GLARMA or
ARIMA, respectively. If the GLARMA/ARIMA model seems to be suitable for modeling
the MIMIC data, then I can create a simulated dataset with the fitted model.**

# Simple ARIMA Simulations

## ARIMA Introduction

An auto-regressive integrated moving average model (ARIMA) is specified by
three order parameters: ($p, d, q$).

*$p$ is the number of autoregressive terms*
The p is the auto-regressive (AR($p$)) component and refers to the use of past
values in the regression equation for the series. The auto-regressive parameter
p specifies the number of lags used in the model. Intuitively, this would be
similar to stating that it is likely to be warm tomorrow if it has been warm the
past $p$ days.

*$d$ is the number of nonseasonal differences*
The $d$ represents the degree of differencing in the integrated (I($d$))
component. Differencing a series involves subtracting its current and previous
values $d$ times. Often, differencing is used to stabilize the series when the
stationarity assumption is not met. Intuitively, this would be similar to
stating that it is likely to be same temperature tomorrow if the difference in
temperature in the last $d$ days has been very small.

*$q$ is the number of moving-averages terms*
A moving average (MA($q$)) component represents the error of the model as a
combination of previous error terms, where $q$ defines the number of terms to
include in the model.

Differencing, autoregressive, and moving average components make up a
non-seasonal ARIMA model which can be written as a linear equation:

$$ Y_t = c + \phi_1y_d{_{t-1}} + \phi_p y_d{_{t-p}}+...+\theta_1 e_{t-1} +
\theta_q e_{t-q} + e_t$$

where $y_d$ is $Y$ differenced $d$ times and $c$ is a constant.

ARIMA models can be also specified through a seasonal structure. In this case,
the model is specified by two sets of order parameters: $(p, d, q)$ as described
above and $(P, D, Q)\_m$ parameters describing the seasonal component of $m$
periods.

ARIMA methodology does have its limitations. These models directly rely on past
values, and therefore work best on long and stable series. Also note that ARIMA
simply approximates historical patterns and therefore does not aim to explain
the structure of the underlying data mechanism.

### Resources

* [A Short Introduction to
   ARIMA](https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials)

* [Time Series: AR, MA,
   ARMA, ARIMA](http://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class16.pdf)

* [Hyndman and Athanasopoulos Forecasting:
   Principles and Practice](https://otexts.com/fpp2/)

## ARIMA Simulations with White Noise

White noise time series can be useful because the stochastic behavior of all
time series can be explained in terms of the white noise model. We simulate
Gaussian white noise, wherein $wt \sim_{iid} N(\mu,\sigma)$. $\mu$ is set
to 0 or a baseline covariate value and $\sigma$ is set to 1 or a baseline
covariate value. We consider three scenarios:

1. $\mu$ is 0 and $\sigma$ is 1, the time series is a function unexplained by
baseline covariates.

2. $\mu$ depends on a baseline covariate and $\sigma$ is 1, the time series is
partially a function of baseline covariates.

3. $\mu$ and $\sigma$ depend on a baseline covariate values, the time series
variation is a function of baseline covariates.

For each of the three scenarios, we simulate $N=500$ subjects each with
$n=1000$ observations from the following models:

1. An autoregressive model of order 1 ($p=1$), where each value of $y$ equals
the previous value times 0.8, plus the white noise.

2. A moving average of order 1 ($q=1$), where each value of $y$ equals the
latest bit of white noise, plus 0.8 times the previous value of white noise.

3. An autoregressive moving average model of order (1, 1), combining the two
above.

4. An ARIMA(1, 1, 1) model that is the cumulative sum of the ARMA(1, 1), so
the first difference of the time series is stationary.

```{r sim_wt_ts-function, echo=FALSE}
set.seed(7194)

# N is the population size
# n is the numer of observations/person
# theta is a vector of length N

sim_wt_ts <- function(N = 500, n = 1000, mu = rep(0,500), sigma = rep(1,500)){
  wn <- matrix(ncol = N, nrow = n)
  ar1 <- matrix(ncol=N, nrow=n)
  ma1 <- matrix(ncol=N, nrow=n)
  arma11 <- matrix(ncol=N,nrow=n)
  arima111 <- matrix(ncol=N,nrow=n)

  for(i in 1:N){
    wn[,i] <- ts(rnorm(n, mean = mu[i], sd = sigma[i]))

    # initialize the first value:
    ar1[1,i] <- ma1[1,i] <- arma11[1,i] <- wn[1,i]

    # loop through and create the 2:nth values:
    for(j in 2:n){
      ar1[j,i] <- ar1[j - 1,i]*0.8 + wn[j,i]
      ma1[j,i] <- wn[j - 1,i]*0.8 + wn[j,i]
      arma11[j,i] <- arma11[j - 1,i]*0.8 + wn[j - 1,i]*0.8 + wn[j,i]
      }
    }

  arima111 <- apply(arma11, 2, cumsum)

  # turn them into time series, and for the last two, "integrate" them via
  # cumulative sum
  ar1 <- ts(ar1)
  ma1 <- ts(ma1)
  arma11 <- ts(arma11)
  arima111 <- ts(arima111)

  return(list(ar1 = ar1,
              ma1 = ma1,
              arma11 = arma11,
              arima111 = arima111))
}
```

```{r, sim_wt_ts-run}
# mu is 0 and sigma is 1
wt_ts_0 <- sim_wt_ts()

# mu is W3 and sigma is 1
W1 <- runif(500, min = -1, max = 1)
W2 <- rbinom(500, prob = plogis(W1), size = 1)
W3 <- W1 + W2
wt_ts_W <- sim_wt_ts(mu = W3)

# mu is W3 and sigma is W4
W4 <- W1 + 1
wt_ts_WW <- sim_wt_ts(mu = W3, sigma = W4)
```

Now we can visualize the various models.

\pagebreak

#### 1. An autoregressive model of order 1 ($p=1$), where each value of $y$ equals the previous value times 0.8, plus the white noise.

\vspace{.1in}

```{r, sim_wt_ts-plot-ar1, fig.align="center", echo=FALSE, out.width= "60%", out.height= "60%"}
plot(wt_ts_0$ar1[,1:4], main = expression(paste("AR(1): ", mu," = 0, ", sigma," = 1")))
plot(wt_ts_W$ar1[,1:4], main = expression(paste("AR(1): ", mu," = ", phi, "(W), ", sigma," = 1")))
plot(wt_ts_WW$ar1[,1:4], main = expression(paste("AR(1): ", mu," = ", phi, "(W), ", sigma," = ", phi, "(W)")))
```

\pagebreak

#### 2. A moving average of order 1 ($q=1$), where each value of $y$ equals the latest bit of white noise, plus 0.8 times the previous value of white noise.

\vspace{.1in}

```{r, sim_wt_ts-plot-ma1, fig.align="center", echo=FALSE, out.width= "60%", out.height= "60%"}
plot(wt_ts_0$ma1[,1:4], main = expression(paste("MA(1): ", mu," = 0, ", sigma," = 1")))
plot(wt_ts_W$ma1[,1:4], main = expression(paste("MA(1): ", mu," = ", phi, "(W), ", sigma," = 1")))
plot(wt_ts_WW$ma1[,1:4], main = expression(paste("MA(1): ", mu," = ", phi, "(W), ", sigma," = ", phi, "(W)")))
```

\pagebreak

#### 3. An autoregressive moving average model of order (1, 1), combining the two above.

\vspace{.1in}

```{r, sim_wt_ts-plot-arma11, fig.align="center", echo=FALSE, out.width= "60%", out.height= "60%"}
plot(wt_ts_0$arma11[,1:4], main = expression(paste("ARMA(1,1): ", mu," = 0, ", sigma," = 1")))
plot(wt_ts_W$arma11[,1:4], main = expression(paste("ARMA(1,1): ", mu," = ", phi, "(W), ", sigma," = 1")))
plot(wt_ts_WW$arma11[,1:4], main = expression(paste("ARMA(1,1): ", mu," = ", phi, "(W), ", sigma," = ", phi, "(W)")))
```

\pagebreak

#### 4. An ARIMA(1, 1, 1) model that is the cumulative sum of the ARMA(1, 1), so the first difference of the time series is stationary.

\vspace{.1in}

```{r, sim_wt_ts-plot-arima111, fig.align="center", echo=FALSE, out.width= "60%", out.height= "60%"}
plot(wt_ts_0$arima111[,1:4], main = expression(paste("ARIMA(1,1,1): ", mu," = 0, ", sigma," = 1")))
plot(wt_ts_W$arima111[,1:4], main = expression(paste("ARIMA(1,1,1): ", mu," = ", phi, "(W), ", sigma," = 1")))
plot(wt_ts_WW$arima111[,1:4], main = expression(paste("ARIMA(1,1,1): ", mu," = ", phi, "(W), ", sigma," = ", phi, "(W)")))
```

Simulations are based on those presented in [free range statistics](http://freerangestats.info/blog/2015/11/21/arima-sims)

\pagebreak

# MIMIC-based Simulation

```{r, mimic}
library(here)
library(tidyverse)
load(here("Data", "data.Rdata"))
source(here("R", "utils_mimic.R"))

df$care_unit <- ifelse(df$care_unit == 1, "MICU",
                  ifelse(df$care_unit == 2, "SICU",
                    ifelse(df$care_unit == 4, "CSRU",
                      ifelse(df$care_unit == 5, "NICU",
                        ifelse(df$care_unit == 6, "CCU", NA)))))

# PROBLEM: some samples are repeated?
# first remove the rows without our outcome of interest
new_df <- df[!with(df, is.na(abpdias) & is.na(abpsys)),]
rs <- rowSums(is.na(new_df))
df_ordered <- new_df[order(new_df$subject_id, new_df$time_and_date, rs), ]
dat <- df_ordered[!duplicated(df_ordered[c('subject_id', 'time_and_date')]), ]

# set time to be continuous
dat <- dat %>%
          group_by(subject_id) %>%
            mutate(init_time_and_date = min(time_and_date)) %>%
              mutate(min_elapsed = as.integer((time_and_date -
                init_time_and_date) / 60)) %>%
                mutate(time_cont = min_elapsed + 1) %>%

# PROBLEM: How to deal with missing values?
colSums(is.na(dat))
# maybe some NA baseline values can be filled in with the non-NA baseline value
subjects <- unique(dat$subject_id)
list_na_control <- lapply(subjects, function(x){
  d <- filter(dat, subject_id == x)
  missing <- c("bmi", "sapsi_first", "sofa_first", "admission_type_descr")
  new_cols <- lapply(missing, function(y){
    index <- which(colnames(d) == y)
    if(length(unique(d[,index])) == 2) {
      lev <- which(!is.na(unique(d[,index])))
      new_vals <- rep(unique(d[,index])[lev], nrow(d))
      new_name <- paste0(y, "_new")
      d <- mutate(d, new_name, new_vals)
      index_new <- which(colnames(d) == new_name)
      return(d[,index_new])
    } else {
      return(d[,index])
    }
  })
  new_dat <- do.call("cbind", new_cols)
  new_dat <- data.frame(time_and_date = d$time_and_date, new_dat)
  new <- merge(d, new_dat, by = "time_and_date")
})

lapply(list_na_control, function(x) grep("_new$", colnames(x)))

dat <- run_class(dat,
                cols_fac = c("subject_id", "gender", "care_unit",
                             "admission_type_descr", "amine", "sedation",
                             "ventilation", "event")
                cols_num = c("periode", "time", "age", "sapsi_first",
                             "sofa_first", "bmi", "los_icu", "los_hospital",
                             "hr", "spo2", "abpsys", "abpdias", "abpmean"))      

```
